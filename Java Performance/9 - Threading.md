# 线程及同步的性能 #

---

## 线程池和ThreadPoolExecutors ##

虽然在程序中可以直接使用Thread类型来进行线程操作，但是更多的情况是使用线程池，尤其是在Java EE应用服务器中，一般会使用若干个线程池来处理来自客户端的请求。Java中对于线程池的支持，来自ThreadPoolExecutor。一些应用服务器也确实是使用的ThreadPoolExecutor来实现线程池。

对于线程池的性能调优，最重要的参数就是线程池的大小。

对于任何线程池而言，它们的工作方式几乎都是相同的：

- 任务被投放到一个队列中(队列的数量不定)
- 线程从队列中取得任务并执行
- 线程完成任务后，继续尝试从队列中取得任务，如果队列为空，那么线程进入等待状态

线程池往往拥有最小和最大线程数：

- 最小线程数，即当任务队列为空时，线程池中最少需要保持的线程数量，这样做是考虑到创建线程是一个相对耗费资源的操作，应当尽可能地避免，当有新任务被投入队列时，总会有线程能够立即对它进行处理。
- 最大线程数，当需要处理的任务过多时，线程池能够拥有的最大线程数。这样是为了保证不会有过多的线程被创建出来，因为线程的运行需要依赖于CPU资源和其它各种资源，当线程过多时，反而会降低性能。

在ThreadPoolExecutor和其相关的类型中，最小线程数被称为线程池核心规模(Core Pool Size)，在其它Java应用服务器的实现中，这个数量也许被称为最小线程数(MinThreads)，但是它们的概念是相同的。

但是在对线程池进行规模变更(Resizing)的时候，ThreadPoolExecutor和其它线程池的实现也许存在的很大的差别。

一个最简单的情况是：当有新任务需要被执行，且当前所有的线程都被占用时，ThreadPoolExecutor和其它实现通常都会新创建一个线程来执行这个新任务(直到达到了最大线程数)。

### 设置最大线程数 ###

最合适的最大线程数该怎么确定，依赖以下两个方面：
- 任务的特征
- 计算机的硬件情况

为了方便讨论，下面假设JVM有4个可用的CPU。那么任务也很明确，就是要最大程度地“压榨”它们的资源，千方百计的提高CPU的利用率。

那么，最大线程数最少需要被设置成4，因为有4个可用的CPU，意味着最多能够并行地执行4个任务。当然，垃圾回收(Garbage Collection)在这个过程中也会造成一些影响，但是它们往往不需要使用整个CPU。一个例外是，当使用了CMS或者G1垃圾回收算法时，需要有足够的CPU资源进行垃圾回收。

那么是否有必要将线程数量设置的更大呢？这就取决于任务的特征了。

假设当任务是计算密集型的，意味着任务不需要执行IO操作，例如读取数据库，读取文件等，因此它们不涉及到同步的问题，任务之间完全是独立的。比如使用一个批处理程序读取Mock数据源的数据，测试在不线程池拥有不同线程数量时的性能，得到下表：

| 线程数 | 执行时间(秒) | 基线百分比 |
| --- | --- | --- |
| 1 | 255.6 | 100% |
| 2 | 134.8 | 52.7% |
| 4 | 77.0 | 30.1% |
| 8 | 81.7 | 31.9% |
| 16 | 85.6 | 33.5% |

从上面中得到一些结论：

- 当线程数为4时，达到最优性能，再增加线程数量时并没有更好的性能，因为此时CPU的利用率已经达到了最高，在增加线程只会增加线程之间争夺CPU资源的行为，因此反而降低了性能。
- 即使在CPU利用率达到最高时，基线百分比也不是理想中的25%，这是因为虽然在程序运行过程中，CPU资源并不是只被应用程序线程独享的，一些后台线程有时也会需要CPU资源，比如GC线程和系统的一些线程等。

当计算是通过Servlet触发的时候，性能数据是下面这个样子的(Load Generator会同时发送20个请求)：

| 线程数 | 每秒操作数(OPS) | 基线百分比 |
| --- | --- | --- |
| 4 | 77.43 | 100% |
| 8 | 75.93 | 98.8% |
| 16 | 71.65 | 92.5% |
| 32 | 69.34 | 89.5% |
| 64 | 60.44 | 78.1% |

从上表中可以得到的结论：

- 在线程数量为4时，性能最优。因为此任务的类型是计算密集型的，只有4个CPU，因此线程数量为4时，达到最优情况。
- 随着线程数量逐渐增加，性能下降，因为线程之间会互相争夺CPU资源，造成频繁切换线程执行上下文环境，而这些切换只会浪费CPU资源。
- 性能下降的速度并不明显，这也是因为任务类型是计算密集型的缘故，如果性能瓶颈不是CPU提供的计算资源，而是外部的资源，如数据库，文件操作等，那么增加线程数量带来的性能下降也许会更加明显。

下面，从Client的角度考虑一下问题，并发Client的数量对于Server的响应时间会有什么影响呢？还是同样地环境，当并发Client数量逐渐增加时，响应时间会如下发生变化：

| 并发Client线程数 | 平均响应时间(秒) | 基线百分比 |
| --- | --- | --- |
| 1 | 0.05 | 100% |
| 2 | 0.05 | 100% |
| 4 | 0.05 | 100% |
| 6 | 0.076 | 152% |
| 8 | 0.104 | 208% |
| 16 | 0.212 | 424% |
| 32 | 0.437 | 874% |
| 64 | 0.909 | 1818% |

因为任务类型是计算密集型的，当并发Client数量时1，2，4时，平均响应时间都是最优的，然而当出现多余4个Client时，性能会随着Client的增加发生显著地下降。

当Client数量增加时，你也许会想通过增加服务端线程池的线程数量来提高性能，可是在CPU密集型任务的情况下，这么做只会降低性能。因为系统的瓶颈就是CPU资源，冒然增加线程池的线程数量只会让对于这种资源的竞争更加激烈。

所以，在面对性能方面的问题时。第一步永远是**了解系统的瓶颈**在哪里，这样才能够有的放矢。如果冒然进行所谓的“调优”，让对瓶颈资源的竞争更加激烈，那么带来的只会是性能的进一步下降。相反，如果让对瓶颈资源的竞争变的缓和，那么性能通常则会提高。

在上面的场景中，如果从ThreadPoolExecutor的角度进行考虑，那么在任务队列中一直会有任务处于挂起(Pending)的状态(因为Client的每个请求对应的就是一个任务)，而所有的可用线程都在工作，CPU正在满负荷运转。这个时候添加线程池的线程数量，让这些添加的线程领取一些挂起的任务，会发生什么事情呢？这时带来的只会是线程之间对于CPU资源的争夺更加激烈，降低了性能。

### 设置最小线程数 ###

设置了最大线程数之后，还需要设置最小线程数。对于绝大部分场景，将它设置的和最大线程数相等就可以了。

将最小线程数设置的小于最大线程数的初衷是为了节省资源，因为每多创建一个线程都会耗费一定量的资源，尤其是线程栈所需要的资源。但是在一个系统中，针对硬件资源以及任务特点选定了最大线程数之后，就表示这个系统总是会利用这些线程的，那么还不如在一开始就让线程池把需要的线程准备好。然而，把最小线程数设置的小于最大线程数所带来的影响也是非常小的，一般都不会察觉到有什么不同。

在批处理程序中，最小线程数是否等于最大线程数并不重要。因为最后线程总是需要被创建出来的，所以程序的运行时间应该几乎相同。对于服务器程序而言，影响也不大，但是一般而言，线程池中的线程在“热身”阶段就应该被创建出来，所以这也是为什么建议将最小线程数设置的等于最大线程数的原因。

在一些场景中，也需要要设置一个不同的最小线程数。比如当一个系统最大需要同时处理2000个任务，而平均任务数量只是20个情况下，就需要将最小线程数设置成20，而不是等于其最大线程数2000。此时如果还是将最小线程数设置的等于最大线程数的话，那么闲置线程(Idle Thread)占用的资源就比较可观了，尤其是当使用了ThreadLocal类型的变量时。

### 线程池任务数量(Thread Pool Task Sizes) ###

线程池有一个列表或者队列的数据结构来存放需要被执行的任务。显然，在某些情况下，任务数量的增长速度会大于其被执行的速度。如果这个任务代表的是一个来自Client的请求，那么也就意味着该Client会等待比较长的时间。显然这是不可接受的，尤其对于提供Web服务的服务器程序而言。

所以，线程池会有机制来限制列表/队列中任务的数量。但是，和设置最大线程数一样，并没有一个放之四海而皆准的最优任务数量。这还是要取决于具体的任务类型和不断的进行性能测试。

对于ThreadPoolExecutor而言，当任务数量达到最大时，再尝试增加新的任务就会失败。ThreadPoolExecutor有一个rejectedExecution方法用来拒绝该任务。这会导致应用服务器返回一个HTTP状态码500，当然这种信息最好以更友好的方式传达给Client，比如解释一下为什么你的请求被拒绝了。

### 定制ThreadPoolExecutor ###

线程池在同时满足以下三个条件时，就会创建一个新的线程：
- 有任务需要被执行
- 当前线程池中所有的线程都处于工作状态
- 当前线程池的线程数没有达到最大线程数

至于线程池会如何创建这个新的线程，则是根据任务队列的种类：

- 任务队列是 SynchronousQueue
	这个队列的特点是，它并不能放置任何任务在其队列中，当有任务被提交时，使用SynchronousQueue的线程池会立即为该任务创建一个线程(如果线程数量没有达到最大时，如果达到了最大，那么该任务会被拒绝)。这种队列适合于当任务数量较小时采用。也就是说，在使用这种队列时，未被执行的任务没有一个容器来暂时储存。

- 任务队列是 无限队列(Unbound Queue)
	无界限的队列可以是诸如LinkedBlockingQueue这种类型，在这种情况下，任何被提交的任务都不会被拒绝。但是线程池会忽略最大线程数这一参数，意味着线程池的最大线程数就变成了设置的最小线程数。所以在使用这种队列时，通常会将最大线程数设置的和最小线程数相等。这就相当于使用了一个固定了线程数量的线程池。

- 任务队列是 有限队列(Bounded Queue)
	当使用的队列是诸如ArrayBlockingQueue这种有限队列的时候，来决定什么时候创建新线程的算法就相对复杂一些了。比如，最小线程数是4，最大线程数是8，任务队列最多能够容纳10个任务。在这种情况下，当任务逐渐被添加到队列中，直到队列被占满(10个任务)，此时线程池中的工作线程仍然只有4个，即最小线程数。只有当仍然有任务希望被放置到队列中的时候，线程池才会新创建一个线程并从队列头部拿走一个任务，以腾出位置来容纳这个最新被提交的任务。

关于如何定制ThreadPoolExecutor，遵循KISS原则(Keep It Simple, Stupid)就好了。比如将最大线程数和最小线程数设置的相等，然后根据情况选择有限队列或者无限队列。

#### 总结 ####

1. 线程池是对象池的一个有用的例子，它能够节省在创建它们时候的资源开销。并且线程池对系统中的线程数量也起到了很好的限制作用。
2. 线程池中的线程数量必须仔细的设置，否则冒然增加线程数量只会带来性能的下降。
3. 在定制ThreadPoolExecutor时，遵循KISS原则，通常情况下会提供最好的性能。

### ForkJoinPool ###

在Java 7中引入了一种新的线程池：ForkJoinPool。

它同ThreadPoolExecutor一样，也实现了Executor和ExecutorService接口。它使用了一个无限队列来保存需要执行的任务，而线程的数量则是通过构造函数传入，如果没有向构造函数中传入希望的线程数量，那么当前计算机可用的CPU数量会被设置为线程数量作为默认值。

ForkJoinPool主要用来使用分治法(Divide-and-Conquer Algorithm)来解决问题。典型的应用比如快速排序算法。这里的要点在于，ForkJoinPool需要使用相对少的线程来处理大量的任务。比如要对1000万个数据进行排序，那么会将这个任务分割成两个500万的排序任务和一个针对这两组500万数据的合并任务。以此类推，对于500万的数据也会做出同样的分割处理，到最后会设置一个阈值来规定当数据规模到多少时，停止这样的分割处理。比如，当元素的数量小于10时，会停止分割，转而使用插入排序对它们进行排序。

那么到最后，所有的任务加起来会有大概2000000+个。问题的关键在于，对于一个任务而言，只有当它所有的子任务完成之后，它才能够被执行。

所以当使用ThreadPoolExecutor时，使用分治法会存在问题，因为ThreadPoolExecutor中的线程无法像任务队列中再添加一个任务并且在等待该任务完成之后再继续执行。而使用ForkJoinPool时，就能够让其中的线程创建新的任务，并挂起当前的任务，此时线程就能够从队列中选择子任务执行。

比如，我们需要统计一个double数组中小于0.5的元素的个数，那么可以使用ForkJoinPool进行实现如下：

```java
public class ForkJoinTest {
	private double[] d;
	private class ForkJoinTask extends RecursiveTask<Integer> {
		private int first;
		private int last;
		public ForkJoinTask(int first, int last) {
			this.first = first;
			this.last = last;
		}
		protected Integer compute() {
			int subCount;
			if (last - first < 10) {
				subCount = 0;
				for (int i = first; i <= last; i++) {
					if (d[i] < 0.5)
						subCount++;
					}
				}
			else {
				int mid = (first + last) >>> 1;
				ForkJoinTask left = new ForkJoinTask(first, mid);
				left.fork();
				ForkJoinTask right = new ForkJoinTask(mid + 1, last);
				right.fork();
				subCount = left.join();
				subCount += right.join();
			}
			return subCount;
		}
	}
	public static void main(String[] args) {
		d = createArrayOfRandomDoubles();
		int n = new ForkJoinPool().invoke(new ForkJoinTask(0, 9999999));
		System.out.println("Found " + n + " values");
	}
}
```

以上的关键是fork()和join()方法。在ForkJoinPool使用的线程中，会使用一个内部队列来对需要执行的任务以及子任务进行操作来保证它们的执行顺序。

那么使用ThreadPoolExecutor或者ForkJoinPool，会有什么性能的差异呢？

首先，使用ForkJoinPool能够使用数量有限的线程来完成非常多的具有父子关系的任务，比如使用4个线程来完成超过200万个任务。但是，使用ThreadPoolExecutor时，是不可能完成的，因为ThreadPoolExecutor中的Thread无法选择优先执行子任务，需要完成200万个具有父子关系的任务时，也需要200万个线程，显然这是不可行的。

当然，在上面的例子中，也可以不使用分治法，因为任务之间的独立性，可以将整个数组划分为几个区域，然后使用ThreadPoolExecutor来解决，这种办法不会创建数量庞大的子任务。代码如下：

```java
public class ThreadPoolTest {
	private double[] d;
	private class ThreadPoolExecutorTask implements Callable<Integer> {
		private int first;
		private int last;
		public ThreadPoolExecutorTask(int first, int last) {
			this.first = first;
			this.last = last;
		}
		public Integer call() {
			int subCount = 0;
			for (int i = first; i <= last; i++) {
				if (d[i] < 0.5) {
					subCount++;
				}
			}
			return subCount;
		}
	}
	public static void main(String[] args) {
		d = createArrayOfRandomDoubles();
		ThreadPoolExecutor tpe = new ThreadPoolExecutor(4, 4, Long.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue());
		Future[] f = new Future[4];
		int size = d.length / 4;
		for (int i = 0; i < 3; i++) {
			f[i] = tpe.submit(new ThreadPoolExecutorTask(i * size, (i + 1) * size - 1);
		}
		f[3] = tpe.submit(new ThreadPoolExecutorTask(3 * size, d.length - 1);
		int n = 0;
		for (int i = 0; i < 4; i++) {
			n += f.get();
		}
		System.out.println("Found " + n + " values");
	}
}
```

在分别使用ForkJoinPool和ThreadPoolExecutor时，它们处理这个问题的时间如下：

| 线程数 | ForkJoinPool | ThreadPoolExecutor |
| --- | --- | --- |
| 1 | 3.2s | 0.31s |
| 4 | 1.9s | 0.15s |

对执行过程中的GC同样也进行了监控，发现在使用ForkJoinPool时，总的GC时间花去了1.2s，而ThreadPoolExecutor并没有触发任何的GC操作。这是因为在ForkJoinPool的运行过程中，会创建大量的子任务。而当他们执行完毕之后，会被垃圾回收。反之，ThreadPoolExecutor则不会创建任何的子任务，因此不会导致任何的GC操作。

ForkJoinPool的另外一个特性是它能够实现工作窃取(Work Stealing)，在该线程池的每个线程中会维护一个队列来存放需要被执行的任务。当线程自身队列中的任务都执行完毕后，它会从别的线程中拿到未被执行的任务并帮助它执行。

可以通过以下的代码来测试ForkJoinPool的Work Stealing特性：

```java
for (int i = first; i <= last; i++) {
	if (d[i] < 0.5) {
		subCount++;
	}
	for (int j = 0; j < d.length - i; j++) {
		for (int k = 0; k < 100; k++) {
			dummy = j * k + i; // dummy is volatile, so multiple writes occur
			d[i] = dummy;
		}
	}
}
```

因为里层的循环次数(j)是依赖于外层的i的值的，所以这段代码的执行时间依赖于i的值。当i = 0时，执行时间最长，而i = last时执行时间最短。也就意味着任务的工作量是不一样的，当i的值较小时，任务的工作量大，随着i逐渐增加，任务的工作量变小。因此这是一个典型的任务负载不均衡的场景。

这时，选择ThreadPoolExecutor就不合适了，因为它其中的线程并不会关注每个任务之间任务量的差异。当执行任务量最小的任务的线程执行完毕后，它就会处于空闲的状态(Idle)，等待任务量最大的任务执行完毕。

而ForkJoinPool的情况就不同了，即使任务的工作量有差别，当某个线程在执行工作量大的任务时，其他的空闲线程会帮助它完成剩下的任务。因此，提高了线程的利用率，从而提高了整体性能。

这两种线程池对于任务工作量不均衡时的执行时间：

| 线程数 | ForkJoinPool | ThreadPoolExecutor |
| --- | --- | --- |
| 1 | 54.5s | 53.3s |
| 4 | 16.6s | 24.2s |

注意到当线程数量为1时，两者的执行时间差异并不明显。这是因为总的计算量是相同的，而ForkJoinPool慢的那一秒多是因为它创建了非常多的任务，同时也导致了GC的工作量增加。

当线程数量增加到4时，执行时间的区别就较大了，ForkJoinPool的性能比ThreadPoolExecutor好将近50%，可见Work Stealing在应对任务量不均衡的情况下，能够保证资源的利用率。

所以一个结论就是：当任务的任务量均衡时，选择ThreadPoolExecutor往往更好，反之则选择ForkJoinPool。

另外，对于ForkJoinPool，还有一个因素会影响它的性能，就是停止进行任务分割的那个阈值。比如在之前的快速排序中，当剩下的元素数量小于10的时候，就会停止子任务的创建。下表显示了在不同阈值下，ForkJoinPool的性能：

| 线程数 | ForkJoinPool |
| --- | --- |
| 20 | 17.8s |
| 10 | 16.6s |
| 5 | 15.6s |
| 1 | 16.8s |

可以发现，当阈值不同时，对于性能也会有一定影响。因此，在使用ForkJoinPool时，对此阈值进行测试，使用一个最合适的值也有助于整体性能。

#### 自动并行化(Automatic Parallelization) ####

在Java 8中，引入了自动并行化的概念。它能够让一部分Java代码自动地以并行的方式执行，前提是使用了ForkJoinPool。

Java 8为ForkJoinPool添加了一个通用线程池，这个线程池用来处理那些没有被显式提交到任何线程池的任务。它是ForkJoinPool类型上的一个静态元素，它拥有的默认线程数量等于运行计算机上的处理器数量。

当调用Arrays类上添加的新方法时，自动并行化就会发生。比如用来排序一个数组的并行快速排序，用来对一个数组中的元素进行并行遍历。自动并行化也被运用在Java 8新添加的Stream API中。

比如下面的代码用来遍历列表中的元素并执行需要的计算：

```java
Stream<Integer> stream = arrayList.parallelStream();
stream.forEach(a -> {
	String symbol = StockPriceUtils.makeSymbol(a);
	StockPriceHistory sph = new StockPriceHistoryImpl(symbol, startDate, endDate, entityManager);
});
```

对于列表中的元素的计算都会以并行的方式执行。forEach方法会为每个元素的计算操作创建一个任务，该任务会被前文中提到的ForkJoinPool中的通用线程池处理。以上的并行计算逻辑当然也可以使用ThreadPoolExecutor完成，但是就代码的可读性和代码量而言，使用ForkJoinPool明显更胜一筹。

对于ForkJoinPool通用线程池的线程数量，通常使用默认值就可以了，即运行时计算机的处理器数量。如果需要调整线程数量，可以通过设置系统属性：`-Djava.util.concurrent.ForkJoinPool.common.parallelism=N`

下面的一组数据用来比较使用ThreadPoolExecutor和ForkJoinPool中的通用线程池来完成上面简单计算时的性能：

| 线程数 | ThreadPoolExecutor(秒) | ForkJoinPool Common Pool(秒) |
| --- | --- | --- |
| 1 | 255.6 | 135.4 |
| 2 | 134.8 | 110.2 |
| 4 | 77.0 | 96.5 |
| 8 | 81.7 | 84.0 |
| 16 | 85.6 | 84.6 |

注意到当线程数为1，2，4时，性能差异的比较明显。线程数为1的ForkJoinPool通用线程池和线程数为2的ThreadPoolExecutor的性能十分接近。

出现这种现象的原因是，forEach方法用了一些小把戏。它会将执行forEach本身的线程也作为线程池中的一个工作线程。因此，即使将ForkJoinPool的通用线程池的线程数量设置为1，实际上也会有2个工作线程。因此在使用forEach的时候，线程数为1的ForkJoinPool通用线程池和线程数为2的ThreadPoolExecutor是等价的。

所以当ForkJoinPool通用线程池实际需要4个工作线程时，可以将它设置成3，那么在运行时可用的工作线程就是4了。

#### 总结 ####

1. 当需要处理递归分治算法时，考虑使用ForkJoinPool。
2. 仔细设置不再进行任务划分的阈值，这个阈值对性能有影响。
3. Java 8中的一些特性会使用到ForkJoinPool中的通用线程池。在某些场合下，需要调整该线程池的默认的线程数量。


## 线程同步(Thread Synchronization) ##

### 同步和Java并发工具 ###

当提到同步(Synchronization)时，也就意味着在某一个时刻，只有一个线程能够执行某个代码片段。为了达到同步的目的，可以使用synchronized关键字，或者使用Lock类，以及java.util.concurrent和java.util.concurrent.atomic包中的相关类型。

严格说来，原子类型并不使用同步(Synchronization)，它们使用的是一个叫做Compare and Swap(CAS)的CPU指令，而同步则是为了保证对一个资源的独占性访问。使用CAS指令的线程并没有独占它需要访问的资源。

这两种方式(同步和CAS)在性能上的差异在后面会进行讨论。但是，尽管CAS并不是等同于同步，它在表现形式上还是和同步非常类似，至少在开发者眼里是这样。

### 同步的代价 ###

被同步的代码对性能有两个方面的影响：

1. 在同步代码块中消耗的时间对应用的可扩展性有影响
2. 获取同步代码块的锁需要消耗一些CPU资源，因此会影响性能

**同步和可扩展性**

当一个应用中存在多个线程时，该应用的加速指数(Speedup)可以通过阿姆达尔定律(Amdahl's Law)来计算：

*Speedup = 1 / (( 1 - P ) + P / N)*

P是一个百分比，表示的是程序中以并行方式运行的代码的占比。
N是利用的线程数(假设这些线程总是有可以利用的CPU资源)。

所以根据这个定律，当P=0.8，即并行代码的占比未80%，N=8，即当可用的线程为8(CPU数量也为8，因为每个线程都是随时可执行的)时，该应用的加速指数为3.33。意味着它比纯粹的串行运行要快上3.33倍。同时需要注意的是，这里计算得到的3.33是一个理想值，也就意味着这是最理想的情况，考虑到各种客观因素，实际上的加速达不到这个值。

这个定律的关键变量就是P，当它下降时，也就是应用的并行度下降时，不仅1 - P这部分值会增加，P / N这部分值也会增加，意味着线程带来的性能增加也会因为并行度的下降而受到负面影响。比如，在理想情况下，当P = 1，意味着整个应用都是并行的，此时Speedup的值就等于N，即CPU的个数。但是当P = 0.8，应用中仅有20%的代码是串行的，Speedup的值就从最理想的8降低到了3.33，可见应用的并发度对于程序性能的影响多么大。

**给对象加锁的代价**

除去对可扩展性的影响，同步操作的的影响有两方面：

- 获取同步锁的代价
	如果并没有两个或者两个以上的线程去争夺一个锁，那么这个代价是很小的。对于synchronized关键字和CAS指令，它们也有一些细微的差别。当锁没有竞争时，两者的代价都很小，相比之下，CAS指令的代价更小一些。但是当获取锁存在竞争时，对于使用synchronized关键字的锁获取，对于每个竞争中的线程，其代价是固定的，不会因为竞争线程增加而增加这个代价。比如对于一个线程而言，这个代价是x，那么有10个线程竞争，总代价就是10x；有20个线程竞争，总代价就是20x。对于使用CAS指令的场景，当发生竞争时，代价是不确定的。CAS指令基于一种乐观的策略：一个线程设置了一个变量的值，执行了一些代码，然后确保这个变量的值没有发生变化。如果发生了变化，表示有其它线程也对该变量进行了修改，因此该线程必须再把这段逻辑执行一次。因此，当有多个线程尝试执行这段代码的时候，因为竞争的存在，这段代码也许会被执行若干次，由此带来的性能损失是无法确定的。但是，使用CAS指令的另外一点好处在于，当多个线程同时使用诸如AtomicLong.get()方法时，是不会发生锁的竞争的，因为这是一个只读操作，CAS能够判断这一点，从而减少不必要的获取锁的操作。

- Java内存模型带来的代价
	和其它诸如C++，C语言不一样的是，Java明确地规范了在处理同步问题时，程序该如何对内存进行操作。这个规范适用于CAS指令，传统的基于synchronized关键字的同步以及使用volatile关键字的内存同步。

	从内存的角度出发，同步的目的就是要保证对变量的更新，会被正确的反映到内存中去。这是因为，变量有可能会被暂时存放到寄存器中，而寄存器中的值并不是对所有的线程都可见的。存放在寄存器中的值，总是需要在将来的某个时间被写入到内存中。同步的作用就是让这个“将来的时间点”受代码的控制，从而让变量能够正确地被其它线程访问。简而言之，对于使用synchronized关键字进行同步的场景，就是当代码离开同步代码块的时候，被修改的变量必须被写入到内存中去；对于使用CAS指令的场景，就是当变量被修改后，该变量也会被写入到内存中；对于volatile变量而言，每次它被修改，它在内存中的值都会被更新。

	前面提到了变量在寄存器中的值和在内存中的值也许是不同步的，那么在执行从寄存器到内存的同步时，会执行Register Flushing操作。这个操作的代价根据处理器的不同而不同，但是总体而言，当线程可用的寄存器数量越多时，这个操作的代价就越大。所以，这个操作在大型的计算机上是会消耗一些资源的。

#### 使用volatile的例子 ####
TODO

#### 总结 ####

1. 线程同步有两方面的性能开销：1) 限制了应用的可扩展性；2) 获取锁的代价。
2. Java内存模型对Synchronized代码块，CAS指令以及volatile变量的影响和它们的开销。

### 避免同步 ###

实际上，有两个方法可以避免使用同步：

- 在每个线程中使用不同的对象来避免同步

	在Java类库中，经常为了在多个线程中使用非线程安全的对象，而使用同步代码块。典型的比如Random，NumberFormat类型。但是从本质上而言，在不同的线程中尝试访问这些非线程安全对象就是不对的，因为它们不应该被当做一个共享资源。使用同步代码块只是让这个共享过程变的安全，但是却没有解决根本性的问题：它们不应该被共享。

	但是，如果每个线程在需要使用它们时创建一个实例，这种做法又显得不是那么有必要并且浪费了资源，加重了GC的负担。

	一个更好解决方案是使用ThreadLocal对象：

	```java
	public class Thermometer {
		private static ThreadLocal<NumberFormat> nfLocal = new ThreadLocal<>() {
			public NumberFormat initialValue() {
				NumberFormat nf = NumberFormat.getInstance();
				nf.setMinumumIntegerDigits(2);
				return nf;
			}
		}
		public String toString() {
			NumberFormat nf = nfLocal.get();
			nf.format(...);
		}
	}
	```

	只有当NumberFormat类型的实例被需要时，即上面的toString方法中调用的：`nfLocal.get()`。此时才会为当前线程创建一个该类型的实例。这样做既能够避免了同步，同时也减少了GC的负担。

- 使用基于CAS指令的类型

	严格来说，它并不是避免了同步，而是减少了在使用传统的synchronized代码块时带来的性能下降。比如，当多个线程需要更新一个计数器时，使用传统synchronized代码块和使用基于CAS指令的类型的实现分别是：

	synchronized代码块：
	```java
		private volatile long al = 0;
		public synchronized long doOperation() {
			return al++;
		}
	```

	CAS指令：
	```java
		AtomicLong al = new AtomicLong(0);
		public long doOperation() {
			return al.getAndIncrement();
		}
	```

	下面是关于使用CAS指令的指导方针：

	- 当需要获取的资源不存在竞争时，CAS指令的性能比传统同步稍好一点。(当然，这种情况下根本不需要使用CAS或者传统同步，不使用的话性能会更好一点)
	- 当需要获取的资源存在轻微或者中等程度的竞争时，CAS指令的性能会明显优于传统同步。
	- 当需要获取的资源存在非常激烈的竞争时，传统同步方式在某些情况下反而会有更好的性能。在实践中，这种情况发生在拥有非常多的线程的大型机器上。
	- 当只需要对资源进行读操作时，CAS指令的不会受到竞争的影响。

#### 竞争和volatile变量 ####
TODO

#### Java 8和竞争的原子类型 ####
TODO

#### 总结 ####

1. 避免同步对象的竞争能够有效缓和因为同步带来的性能影响。
2. ThreadLocal对象不存在竞争，当对象不需要被共享时，可以考虑使用它。
3. CAS指令在大多数场合是比传统同步更好的选择，当对象需要被共享时，可以考虑使用它。


### 错误共享 ###

这是一个很少被提及的话题，但是随着多核处理器和并行应用的逐渐普及，由于错误共享而带来的性能问题正在逐渐增加。

CPU处理缓存的方式是导致这一问题的原因。

比如，在以下的代码中：

```java
public class DataHolder {
	public volatile long l1;
	public volatile long l2;
	public volatile long l3;
	public volatile long l4;
}
```

以上的4个long类型变量都会被相邻地存放在内存中。比如l1的内存地址是0xF24, 那么l2和l3的内存地址就分别是0xF28和0xF2C。当一个线程需要处理l2时，它会加载一片内存区域，而不是仅仅代表l2的那4个字节，具体加载多大的内存区域因CPU而异。这片被加载的内存会被放入CPU的缓存中。那么当另一个线程要处理l3时，相同区域的内存又会被加载到另一个CPU的缓存中。

加载邻近区域的这一做法背后的原因在于，如果程序访问了某个对象的一个字段，那么它还有很大的可能性会访问其他的字段。因此，在对其他字段进行加载的时候，由于邻近区域已经被加载到缓存中了，处理器有很大的可能性能够直接从缓存中获取该字段，而不需要再次访问内存。

这种做法的劣势在于，当一个线程更新缓存中的某个变量后，该处理器必须要通知其他的处理器这片内存已经被更新了。从而导致其他的处理器重新对该片内存进行加载。

在以下的程序中，DataHolder对象同时会被几个线程同时使用：

```java
public class ContendedTest extends Thread {
	private static class DataHolder {
		private volatile long l1 = 0;
		private volatile long l2 = 0;
		private volatile long l3 = 0;
		private volatile long l4 = 0;
	}
	private static DataHolder dh = new DataHolder();
	private static long nLoops;
	public ContendedTest(Runnable r) {
		super(r);
	}
	public static void main(String[] args) throws Exception {
		nLoops = Long.parseLong(args[0]);
		ContendedTest[] tests = new ContendedTest[4];
		tests[0] = new ContendedTest(() -> {
			for (long i = 0; i < nLoops; i++) {
				dh.l1 += i;
			}
		});
		tests[1] = new ContendedTest(() -> {
			for (long i = 0; i < nLoops; i++) {
				dh.l2 += i;
			}
		});
		//...similar for 2 and 3...
		long then = System.currentTimeMillis();
		for (ContendedTest ct : tests) {
			ct.start();
		}
		for (ContendedTest ct : tests) {
			ct.join();
		}
		long now = System.currentTimeMillis();
		System.out.println("Duration: " + (now - then) + " ms");
	}
}
```

以上启动了4个线程，每个线程会操作DataHolder中的一个变量。它们之间没有任何竞争，因此这段代码应该执行的非常迅速。但是，事实并不是这样的。因为每次更新DataHolder中的变量时，都会造成处理器中缓存的失效和重新加载。当线程数增加时，意味着参与到计算的处理器也随之增加，那么由于缓存的重新加载造成的性能损失就更严重了。

当上述的nLoops值为十亿时，得到在线程数量不同时的性能：

| 线程数 | 执行时间 |
| --- | --- |
| 1 | 7.1 seconds |
| 2 | 52.1 seconds |
| 3 | 91.0 seconds |
| 4 | 128.3 seconds |

严格说来，错误共享和同步以及volatile变量的使用并不相关。再回顾一下错误共享的原因：

> 当CPU中的缓存中有任何数据被更改时，其他CPU中持有相同区域内存的缓存也会无效，从而导致重新加载的行为。

而在Java内存模型中，对同步，CAS指令以及volatile变量的同步语义也进行了规范。当修改volatile变量时，被修改的值会被立即同步到内存中。因此在上述的例子中使用volatile变量能够保证CPU缓存在每次更改变量的值后都会被重新加载。

当将DataHolder中的变量不再声明为volatile时，应用会运行的相当迅速，无论线程数量是多少，大概7.1秒就能够完成。

解决错误共享不是一件容易的事情，因为你需要了解到运行环境处理器的架构信息，来知道它每次会读取多大的内存区域并缓存到处理器中。当然，也有更好的办法，就是减少对于变量的更新，比如对于上面的例子，可以使用一个局部变量来保存所有更新操作，然后在最后将此局部变量的值写入到volatile变量中，就像下面这样：

```java
tests[0] = new ContendedTest(() -> {
	long temp = dh.l1;
	for (long i = 0; i < nLoops; i++) {
		temp += i;
	}
	dh.l1 = temp;
});
```
这样就避免了频繁地对volatile变量进行更新，从而减少了更新处理器缓存和重新加载内存的次数。

在我本地的测试中(作者并没有提供对使用局部变量时的测试数据)，得到的结果是这个样子的：

| 线程数 | 不使用局部变量(基线百分比) | 使用局部变量(基线百分比) |
| --- | --- | --- |
| 1 | 28.03s(100%) | 1.88 seconds(6.7%) | 
| 4 | 58.21s(207.7%) | 0.92 seconds(3.3%) |

测试环境：Intel Core i3-3220 @ 3.30GHz * 2, 8GB Memory

可以发现当使用了局部变量后，性能有质的提升。同时，使用多线程来完成计算此时也不再是负担(6.7% -> 3.3%)。当不使用局部变量时，由于多线程的引入同时也增加了需要重新加载内存的次数，因此性能反而降低了很多(100% -> 207.7%)。

除了使用局部变量之外，还可以使用变量填充(Variable Padding)这一技术来克服处理器的默认内存加载方式。比如：

```java
public class DataHolder {
	public volatile long l1;
	pubilc long[] dummy1 = new long[128 / 8];
	public volatile long l2;
	pubilc long[] dummy2 = new long[128 / 8];
	public volatile long l3;
	pubilc long[] dummy3 = new long[128 / 8];
	public volatile long l4;
}
```

上面的代码仅仅是对这种技术的原理展示，JVM也许会对它进行优化从而对volatile变量在内存中进行重排列。在使用这种技术时，通常会显著增加对象的空间占用，从而加大GC的负担。所以在使用时需要进行仔细的调优。

在使用它时，如何确定padding的值就成了关键。这个值一般和处理器的架构相关，不是在所有的场合下都能够方便和正确地获取。所以，在Java 8中引入了一个注解类型@Contended(sun.misc.Contended)。然而，这个注解默认只能在JDK的类库中使用，如果需要在应用的类型中使用它，需要启用它：`-XX:-RestrictContended`。这个注解能够让JVM帮助你完成Variable Padding，因为JVM比你更了解运行时处理器的架构信息。

另外，也可以通过使用：`-XX:-EnableContended` 完全禁止JVM的Variable Padding功能，对于JDK类库的使用的@Contended可以通过这种方式禁止。这样做会减小Thread以及ConcurrentHashMap类型的内存空间占用，因为其中使用了@Contended注解来进行变量填充。

#### 总结 ####

1. 错误分享在一些场景下会严重的降低性能(正如上面那组数据显示的那样)。
2. 错误分享不容易发现。当一个循环运行的出乎意料的慢时，考虑它是否中了错误分享的招。
3. 错误分享可以通过两种方式避免，使用局部变量或者使用变量填充。


## JVM线程调优 ##

### 调整线程栈空间 ###

当非常缺少内存时，可以调整线程使用的内存。每个线程都有一个栈，用来记录该线程的调用栈信息。线程中的栈的默认空间是有OS和JVM的版本决定的：

| OS | 32-bit | 64-bit |
| --- | --- | --- |
| Linux | 320 KB | 1 MB |
| Mac OS | N/A | 1 MB |
| Solaris Sparc | 512 KB | 1 MB |
| Solaris X86 | 320 KB | 1 MB |
| Windows | 320 KB | 1 MB |

当栈空间被设置的过小时，可能会因为有较长的调用栈而抛出StackOverflowError。

在64位的JVM中，一般不需要修改这个值，除非内存确实非常吃紧。而在32位的JVM中，可以将这个值从320KB设置成128KB，为了给堆内存腾出更多的空间。

更改线程栈空间的指令：`-Xss=N` 比如：`-Xss=256k`

### 偏见锁(Biased Locking) ###

当锁被多个线程所争夺时，JVM和OS能够选择将锁分配给哪个线程。可以使用一种公平的策略将锁分配给其他线程，或者也可以使用一种不公平(偏见)的策略，比如将锁再分配给上一次拥有该锁的线程。

将锁再次分配给上一次拥有该锁的线程，这样做的合理性在于：由于时间上的连续性，处理器中可能还缓存着和该线程所执行任务相关的数据，因此当线程再次执行时，准备上下文的时间就能够节省下来。当然，使用偏见锁本身需要记录一些相关数据，因此在某些时候反而会对性能有影响。

典型的比如，在很多时候，如果将偏见锁应用在线程池中，那么性能反而会变差。如果一个应用并不需要使用偏见锁来作为锁分配的策略，那么可以通过：`-XX:-UseBiasedLocking` 来禁用它，因为默认是会启用偏见锁的，禁用它会提升些许性能。

### 锁的自旋(Lock Spinning) ###

对于没有得到锁的线程，JVM有两种处理方式：

- 让线程进入一个忙循环(Busy Loop)，待它执行了一些指令后会再次检查需要的锁是否可用。
- 让线程进入一个队列，当需要的锁可用时通知它。此时CPU可以被其它线程使用。

如果处于竞争中的锁只需要被持有一小段时间，那么使用第一种忙循环(也被称为线程自旋(Thread Spinning))的速度会比第二种让线程进入队列的方式快的多。反之，当竞争中的锁会被线程持有较长的时间时，使用第二种方式更优，这能够让CPU的有效利用率更高。

JVM会合理地选择使用哪种处理方式。首先会让线程自旋一段时间，如果还没有得到需要的锁，就会将该线程放入队列中等待，从而让出CPU资源给其它线程。

### 线程优先级 ###

在Java API中，每个线程都可以被设置一个优先级，OS会参考这个值。但是注意OS仅仅是“参考”，并不一定会遵循它。OS会对每个运行中的线程计算一个“当前”优先级。这个计算过程会考虑到设置的优先级，但是它仅仅是众多因素中的一个，其中最重要的因素往往是这个线程已经运行了多长时间。考虑这个因素是为了让每个线程都能够得到运行的机会。所以，无论线程被设置的优先级有多低，它们也总能够得到运行的机会。

另外，设置的线程优先级在不同的OS上的权重是不同的。在基于Unix的系统上，线程的执行时间是主导线程当前优先级的因素，也就是说设置的线程优先级几乎不会被“参考”。而在Windows系统上，设置的线程优先级的权重会稍微高一些。

所以，无论是在哪个OS上，应用的性能都不能依赖于对线程设置优先级。如果某些任务的优先级确实高于另外一些任务，那么这一点需要被应用程序的逻辑来完成，而不是通过设置线程的优先级来完成。

一个办法将任务分配给不同的线程池，然后设置这些线程池的规模。在第十章中会举出一个例子，TODO




















































